# 复制 与 故障转移
&nbsp;&nbsp;Redis 集群中的节点分为主节点和从节点，主节点负责处理槽，从节点负责复制某个主节点，并在被复制的主节点下线时，代替主节点处理命令请求。

## 实现的核心
&nbsp;&nbsp;Core: 集群中的各个节点会通过互相发送消息的方式来交换集群中各个节点的状态信息，并且每个节点都会存储集群的状态信息。
## 设置从节点
```log
   # 命令:
    CLUSTER REPLICATE <node_id>
    << PS: 通过node_id对节点进行标识，所以需要先将节点添加到集群中，再进行操作。
    > 该命令可以让接受命令的节点成为节点<node_id>的从节点，并开始对主节点进行复制。
    >> 一个节点成为从节点，并开始复制某个主节点 这一信息会通过消息发送到集群中的其他节点，最终集群中所有节点都知道这一消息。

    127.0.0.1:6385> cluster nodes
    2aa1d7755903cdd57cd5c4082adbb63795dd7da3 127.0.0.1:6388@16388 master - 0 1686811920252 3 connected
    17e4ecdcde4457dbaf436da921c1d5695817a256 127.0.0.1:6387@16387 master - 0 1686811923363 2 connected 10001-16383
    d95665c74858adf511fb5fb3681cca84bd20eafc 127.0.0.1:6389@16389 master - 0 1686811920000 0 connected
    c48adcf82a3f9996a571a4210f7bba14d3042ac1 127.0.0.1:6386@16386 master - 0 1686811922000 1 connected 5001-10000
    c53d7e0057cac2df31ba922c9defd27df473d7fa 127.0.0.1:6385@16385 myself,master - 0 1686811921000 4 connected 0-5000

    ➜  redis-6.2.5 git:(master) ✗ ./src/redis-cli -c -p 6388
    127.0.0.1:6388> CLUSTER REPLICATE c53d7e0057cac2df31ba922c9defd27df473d7fa
    OK
    127.0.0.1:6388> exit
    ➜  redis-6.2.5 git:(master) ✗ ./src/redis-cli -c -p 6389
    127.0.0.1:6389> CLUSTER REPLICATE c53d7e0057cac2df31ba922c9defd27df473d7fa
    OK
    # 设置从节点后
    127.0.0.1:6385> cluster nodes
    2aa1d7755903cdd57cd5c4082adbb63795dd7da3 127.0.0.1:6388@16388 slave c53d7e0057cac2df31ba922c9defd27df473d7fa 0 1686812086000 4 connected
    17e4ecdcde4457dbaf436da921c1d5695817a256 127.0.0.1:6387@16387 master - 0 1686812088168 2 connected 10001-16383
    d95665c74858adf511fb5fb3681cca84bd20eafc 127.0.0.1:6389@16389 slave c53d7e0057cac2df31ba922c9defd27df473d7fa 0 1686812087129 4 connected
    c48adcf82a3f9996a571a4210f7bba14d3042ac1 127.0.0.1:6386@16386 master - 0 1686812086098 1 connected 5001-10000
    c53d7e0057cac2df31ba922c9defd27df473d7fa 127.0.0.1:6385@16385 myself,master - 0 1686812086000 4 connected 0-5000
    127.0.0.1:6385> 
```

## 故障检测
### 疑似下线状态（PFAIL）
&nbsp;&nbsp;集群中的每个节点都会定期的向其他节点发送PING消息，以此来检测对方是否在线。若接收PING消息的节点没有在规定的时间内回复PONG消息，那么发送PING消息的节点就会标记接收PING消息的节点为疑似下线。

### -> 已下线状态（FAIL）
&nbsp;&nbsp;若集群中半数以上的主节点将某一个主节点x报告为疑似下线状态，那么这个主节点x将会被标记为已下线状态。将主节点x标记为已下线状态的节点会广播FAIL消息到集群中，所有收到这条FAIL消息的节点都会将节点x标记为已下线状态。

## 故障转移
&nbsp;&nbsp;故障转移步骤
1. 从下线主节点的从节点中选出一个从节点a。
2. 节点a执行 SLAVEOF NO ONE命令，成为主节点。
3. 节点a撤销所有对已下线主节点的槽指派，将这些槽全部指向自己。
4. 节点a会广播一条PONG消息到集群，让集群中的其他节点知晓节点a已经成为主节点，并已经接管了已下线主节点所处理的槽。
5. 节点a处理与自己负责的槽相关的命令，故障转移完成。

## 新节点的选取
- 选举委员会成员(谁具有投票权)： 负责处理槽的主节点。
- 谁发起：从节点广播’CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST‘消息到集群中，要求所有接收到该条消息的且拥有投票权的主节点向他进行投票。若主节点返回’CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK‘消息，则说明支持他升级为主节点，反之，则不支持。
- 投票结果: 当从节点支持人数 >= N/2 + 1 时（N为具有投票权的主节点），从节点就可以升级为主节点。 若本次没有结果，那么就会开启下一次选举。

## 已下线节点重新上线
```log
   127.0.0.1:6385> cluster nodes
   2aa1d7755903cdd57cd5c4082adbb63795dd7da3 127.0.0.1:6388@16388 slave c53d7e0057cac2df31ba922c9defd27df473d7fa 0 1686812086000 4 connected
   17e4ecdcde4457dbaf436da921c1d5695817a256 127.0.0.1:6387@16387 master - 0 1686812088168 2 connected 10001-16383
   d95665c74858adf511fb5fb3681cca84bd20eafc 127.0.0.1:6389@16389 slave c53d7e0057cac2df31ba922c9defd27df473d7fa 0 1686812087129 4 connected
   c48adcf82a3f9996a571a4210f7bba14d3042ac1 127.0.0.1:6386@16386 master - 0 1686812086098 1 connected 5001-10000
   c53d7e0057cac2df31ba922c9defd27df473d7fa 127.0.0.1:6385@16385 myself,master - 0 1686812086000 4 connected 0-5000

   # Kill 掉主节点: 127.0.0.1:6385
    > 发现，127.0.0.1:6389 升级为了新的主节点。127.0.0.1:6388成为了新的主节点的从节点
    >> 可以通过终端日志了解流程
   127.0.0.1:6389> cluster nodes
   d95665c74858adf511fb5fb3681cca84bd20eafc 127.0.0.1:6389@16389 myself,master - 0 1686814737000 5 connected 0-5000
   c53d7e0057cac2df31ba922c9defd27df473d7fa 127.0.0.1:6385@16385 master,fail - 1686814720413 1686814717000 4 disconnected
   17e4ecdcde4457dbaf436da921c1d5695817a256 127.0.0.1:6387@16387 master - 0 1686814738091 2 connected 10001-16383
   c48adcf82a3f9996a571a4210f7bba14d3042ac1 127.0.0.1:6386@16386 master - 0 1686814737000 1 connected 5001-10000
   2aa1d7755903cdd57cd5c4082adbb63795dd7da3 127.0.0.1:6388@16388 slave d95665c74858adf511fb5fb3681cca84bd20eafc 0 1686814737059 5 connected

   # 重新启动节点: 127.0.0.1:6385
   > 发现，127.0.0.1:6385 成为了新晋主节点的从节点，可通过终端日志观察
   127.0.0.1:6389> cluster nodes
   d95665c74858adf511fb5fb3681cca84bd20eafc 127.0.0.1:6389@16389 myself,master - 0 1686814962000 5 connected 0-5000
   c53d7e0057cac2df31ba922c9defd27df473d7fa 127.0.0.1:6385@16385 slave d95665c74858adf511fb5fb3681cca84bd20eafc 0 1686814961000 5 connected
   17e4ecdcde4457dbaf436da921c1d5695817a256 127.0.0.1:6387@16387 master - 0 1686814963075 2 connected 10001-16383
   c48adcf82a3f9996a571a4210f7bba14d3042ac1 127.0.0.1:6386@16386 master - 0 1686814962036 1 connected 5001-10000
   2aa1d7755903cdd57cd5c4082adbb63795dd7da3 127.0.0.1:6388@16388 slave d95665c74858adf511fb5fb3681cca84bd20eafc 0 1686814961011 5 connected
```
